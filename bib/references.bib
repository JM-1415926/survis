@article{Beck2016Visual,
  abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi = {10.1109/TVCG.2015.2467757},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number = {01},
  publisher = {IEEE},
  volume = {22},
  series = {TVCG},
  title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  year = {2016}
}
@article{Qiao2022Joint,
  abstract = {In this paper, we propose a joint model for named entity recognition and relation extraction based on BERT, named BERT-BILSTM-LSTM. It improves upon the LSTM-LSTM-Bias model by incorporating contextualized embeddings via BERT, enabling better handling of polysemous words. The model achieves improved performance on both a standard dataset (NYT) and a newly constructed agricultural dataset (AgriRelation). Experimental results show that the model is effective on both large and small datasets, with significant gains in F1 score over baseline models.},
  author = {Qiao, Bo and Zou, Zhuoyang and Huang, Yu and Fang, Kui and Zhu, Xinghui and Chen, Yiming},
  doi = {10.1007/s00521-021-05815-z},
  journal = {Neural Computing and Applications},
  keywords = {entity recognition, relation extraction, BERT, agricultural knowledge graph, joint extraction},
  number = {7},
  pages = {3471--3481},
  publisher = {Springer},
  volume = {34},
  title = {A Joint Model for Entity and Relation Extraction Based on BERT},
  url = {https://doi.org/10.1007/s00521-021-05815-z},
  year = {2022}
}
@article{Wang2025Joint,
  abstract = {Entity relation extraction is a key technology for structured knowledge acquisition. To address challenges such as overlapping triplets and inadequate semantic encoding, this paper proposes a supervised joint extraction model based on multi-feature semantic fusion. The model integrates entity mask embeddings and contextual embeddings using BERT as the encoder and employs a parallel decoding strategy to extract sets of relational triples. Experiments on NYT and WebNLG show that the method achieves superior F1 scores compared to baselines.},
  author = {Wang, Ting and Yang, Wenjie and Wu, Tao and Yang, Chuan and Liang, Jiaying and Wang, Hongyang and Li, Jia and Xiang, Dong and Zhou, Zheng},
  doi = {10.1007/s10844-024-00871-y},
  journal = {Journal of Intelligent Information Systems},
  keywords = {joint extraction, overlapping triplets, multi-feature fusion, BERT, set prediction, entity mask},
  number = {1},
  pages = {21--42},
  publisher = {Springer},
  volume = {63},
  title = {Joint Entity and Relation Extraction with Fusion of Multi-feature Semantics},
  url = {https://doi.org/10.1007/s10844-024-00871-y},
  year = {2025}
}
@article{Liang2022Seq2SeqRE,
  abstract = {This paper proposes Seq2Seq-RE, a novel model for joint entity and relation extraction using gate graph neural networks (GGNNs) within a sequence-to-sequence framework. By incorporating various graph edge types such as dependency, forward, backward, and self-links, the model captures complex relational structures and improves the handling of overlapping tuples. Experiments on NYT29 and NYT24 demonstrate its superiority over strong baselines.},
  author = {Liang, Zeyu and Du, Junping},
  doi = {10.1016/j.neucom.2022.05.074},
  journal = {Neurocomputing},
  keywords = {sequence-to-sequence, gate graph neural networks, joint extraction, relation extraction, overlapping relations},
  pages = {480--488},
  publisher = {Elsevier},
  volume = {501},
  title = {Sequence to Sequence Learning for Joint Extraction of Entities and Relations},
  url = {https://doi.org/10.1016/j.neucom.2022.05.074},
  year = {2022}
}
@inproceedings{Shang2022DirectRel,
  abstract = {This paper introduces DirectRel, a novel model that transforms relational triple extraction into a bipartite graph linking problem. By generating candidate entities and linking them directly via a learned relation-specific matrix, the model avoids error accumulation from entity boundary identification. Experiments on NYT and WebNLG demonstrate superior performance over state-of-the-art baselines.},
  author = {Shang, Yu-Ming and Huang, Heyan and Sun, Xin and Wei, Wei and Mao, Xian-Ling},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  title = {Relational Triple Extraction: One Step is Enough},
  year = {2022},
  organization = {IJCAI},
  keywords = {triple extraction, bipartite graph linking, joint learning, error accumulation, DirectRel},
  url = {https://arxiv.org/abs/2205.05270}
}
@inproceedings{Xue2024AutoRE,
  abstract = {AutoRE is an end-to-end document-level relation extraction (DocRE) model designed to address the limitations of existing sentence-level RE approaches and prompt-based LLM methods. It introduces a novel pipeline paradigm called Relation-Head-Facts (RHF), avoids embedding large relation sets into prompts, and employs parameter-efficient fine-tuning via QLoRA. AutoRE achieves state-of-the-art results on the Re-DocRED dataset, surpassing previous methods by over 9% F1.},
  author = {Xue, Lilong and Zhang, Dan and Dong, Yuxiao and Tang, Jie},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  pages = {211--220},
  title = {AutoRE: Document-Level Relation Extraction with Large Language Models},
  year = {2024},
  organization = {ACL},
  url = {https://github.com/THUDM/AutoRE},
  keywords = {document-level relation extraction, large language models, QLoRA, prompt engineering, fine-tuning}
}
@inproceedings{Papaluca2024ZeroShotTE,
  abstract = {This work investigates the zero- and few-shot triplet extraction (TE) capabilities of various large language models (LLMs). A dynamic pipeline is proposed to retrieve relevant contextual triplets or examples from a knowledge base (KB) to augment LLM prompts. Experiments show that KB-augmented prompts significantly enhance performance, making even smaller models competitive with traditional BiLSTM-based TE systems on WebNLG and NYT datasets.},
  author = {Papaluca, Andrea and Krefl, Daniel and Rodríguez Méndez, Sergio J. and Lensky, Artem and Suominen, Hanna},
  booktitle = {Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)},
  pages = {12--23},
  title = {Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models},
  year = {2024},
  organization = {ACL},
  url = {https://github.com/BrunoLiegiBastonLiegi/KG-TE-with-LLMs},
  keywords = {triplet extraction, zero-shot learning, few-shot learning, knowledge graphs, large language models}
}
@inproceedings{Hu2025LLMERE,
  abstract = {LLMERE is a large language model-based framework for event relation extraction (ERE) that addresses challenges of time inefficiency, limited coverage, and lack of rationale. It reformulates ERE as a question-answering task with multiple answers and uses a document partitioning strategy. The model also outputs rationales based on event coreference and transitive chains. Experimental results on MAVEN-ERE, MATRES, and HiEve datasets show that LLMERE outperforms existing baselines.},
  author = {Hu, Zhilei and Li, Zixuan and Jin, Xiaolong and Bai, Long and Guo, Jiafeng and Cheng, Xueqi},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  pages = {7484--7496},
  title = {Large Language Model-Based Event Relation Extraction with Rationales},
  year = {2025},
  organization = {ACL},
  url = {https://github.com/HerbertHu/LLMERE},
  keywords = {event relation extraction, large language models, rationales, document partitioning, question answering}
}
@inproceedings{Zhong2021PURE,
  abstract = {This paper revisits entity and relation extraction using a pipelined model called PURE, which trains two independent encoders for entities and relations respectively. Despite its simplicity, PURE achieves state-of-the-art results on ACE04, ACE05, and SciERC benchmarks, surpassing joint models by 1.7–2.8% in relation F1 score. The study highlights the benefits of using distinct contextual encoders, early fusion of entity features into relation modeling, and an efficient batching approximation with up to 16× speedup at inference.},
  author = {Zhong, Zexuan and Chen, Danqi},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {50--61},
  title = {A Frustratingly Easy Approach for Entity and Relation Extraction},
  year = {2021},
  organization = {ACL},
  url = {https://github.com/princeton-nlp/PURE},
  keywords = {relation extraction, entity extraction, pipelined model, BERT, ACE05, SciERC}
}
@inproceedings{Xie2022EIDER,
  abstract = {EIDER is an evidence-enhanced framework for document-level relation extraction. It uses a lightweight evidence extraction module trained jointly with relation extraction, and fuses predictions from both full-document and extracted-evidence views at inference. This improves performance and efficiency without relying heavily on gold evidence labels. EIDER achieves state-of-the-art results on DocRED, CDR, and GDA benchmarks, especially on multi-sentence and coreference-intensive relations.},
  author = {Xie, Yiqing and Shen, Jiaming and Li, Sha and Mao, Yuning and Han, Jiawei},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  pages = {257--268},
  title = {EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion},
  year = {2022},
  organization = {ACL},
  url = {https://github.com/Veronicium/Eider},
  keywords = {document-level RE, evidence extraction, fusion inference, DocRED, multi-hop reasoning}
}
@inproceedings{Tan2022KDDocRE,
  abstract = {This paper presents a semi-supervised framework for document-level relation extraction using adaptive focal loss and knowledge distillation. It introduces axial attention for two-hop reasoning, a new loss function to mitigate class imbalance, and a teacher-student model to utilize distantly supervised data. The method surpasses prior baselines on DocRED and HacRED by 1.36 F1 and 1.46 Ign_F1 scores, offering a robust solution for long-tail and low-resource DocRE.},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  pages = {1672--1681},
  title = {Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation},
  year = {2022},
  organization = {ACL},
  url = {https://github.com/tonytan48/KD-DocRE},
  keywords = {document-level RE, focal loss, knowledge distillation, axial attention, long-tail learning}
}
